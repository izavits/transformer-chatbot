{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provides utility functions to load the input dataset and process it as needed for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import configparser\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "__author__ = \"ilias Zavitsanos\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"ilias Zavitsanos\"\n",
    "__email__ = \"izavits@gmail.com\"\n",
    "__status__ = \"Research Ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    \"\"\"Preprocess the input line by removing special characters.\"\"\"\n",
    "    line = line.lower().strip()\n",
    "    line = re.sub(r\"([?.!,])\", r\" \\1 \", line)\n",
    "    line = re.sub(r'[\" \"]+', \" \", line)\n",
    "    line = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", line)\n",
    "    line = line.strip()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    \"\"\"Load the dataset\"\"\"\n",
    "    inputs, outputs = [], []\n",
    "    # Input data is not valid json to load at once\n",
    "    with open(datafile) as f:\n",
    "        lines = f.readlines()\n",
    "    data = [json.loads(l) for l in lines]\n",
    "    for d in data:\n",
    "        inputs += [preprocess(i) for (x, i) in enumerate(d['turns']) if x % 2 != 0]\n",
    "        outputs += [preprocess(i) for (x, i) in enumerate(d['turns']) if x % 2 == 0 and x != 0]\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(ins, outs):\n",
    "    \"\"\"Build a topkenizer using Tensorflow's SubwordTextEncoder.\"\"\"\n",
    "    tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(ins + outs, target_vocab_size=2 ** 13)\n",
    "    start_token = [tokenizer.vocab_size]\n",
    "    end_token = [tokenizer.vocab_size + 1]\n",
    "    vocabulary_size = tokenizer.vocab_size + 2\n",
    "    return tokenizer, start_token, end_token, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inputs, outputs):\n",
    "    \"\"\"Tokenize and pad data.\"\"\"\n",
    "    tokenized_ins, tokenized_outs = [], []\n",
    "    data_tokenizer, START_TOKEN, END_TOKEN, VOCAB_SIZE = build_tokenizer(inputs, outputs)\n",
    "    for (question, answer) in zip(inputs, outputs):\n",
    "        question = START_TOKEN + data_tokenizer.encode(question) + END_TOKEN\n",
    "        answer = START_TOKEN + data_tokenizer.encode(answer) + END_TOKEN\n",
    "        tokenized_ins.append(question)\n",
    "        tokenized_outs.append(answer)\n",
    "    tokenized_ins = tf.keras.preprocessing.sequence.pad_sequences(tokenized_ins, maxlen=80, padding='post')\n",
    "    tokenized_outs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outs, maxlen=80, padding='post')\n",
    "    return tokenized_ins, tokenized_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input(questions, answers):\n",
    "    \"\"\"Construct the input for the model.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('../config.ini')\n",
    "    BATCH_SIZE = int(config['MODEL']['BatchSize'])\n",
    "    BUFFER_SIZE = int(config['MODEL']['BufferSize'])\n",
    "    # Use the tensorflow data API to exploit caching and prefetching features\n",
    "    # Use teacher - forcing: pass the true output to the next step\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'inputs': questions,\n",
    "            'dec_inputs': answers[:, :-1]\n",
    "        },\n",
    "        {\n",
    "            'outputs': answers[:, 1:]\n",
    "        },\n",
    "    ))\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
