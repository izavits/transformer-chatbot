{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provides the functionality of the model. The model is a Transformer, a neural network architecture based on self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "__author__ = \"ilias Zavitsanos\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"ilias Zavitsanos\"\n",
    "__email__ = \"izavits@gmail.com\"\n",
    "__status__ = \"Research Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_weights(query, key, value, mask):\n",
    "    \"\"\"Calculate attention weights based on the\n",
    "    scaled dot-product attention function.\"\"\"\n",
    "    qk = tf.matmul(query, key, transpose_b=True)\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = qk / tf.math.sqrt(depth)\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "    weights = tf.nn.softmax(logits, axis=-1)\n",
    "    output = tf.matmul(weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    \"\"\"Mask pad tokens in order not to treat padding as input.\"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    \"\"\"Mask future tokens in a sequence.\"\"\"\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention mechanism\n",
    "\n",
    "The multi-head attentio consist of:\n",
    "\n",
    "- Linear layers and split into heads\n",
    "- Scaled dot-product attention\n",
    "- Heads concatenation\n",
    "- Final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi head attention consists of:\n",
    "    - Linear layers and split into heads\n",
    "    - Scaled dot-product attention\n",
    "    - Heads concatenation\n",
    "    - Final linear layer\n",
    "\n",
    "    Args:\n",
    "        d_model (int): dimension of embedding size and the model data flow\n",
    "        num_heads (int): number of heads in multi-head attention\n",
    "        name (str)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        \"\"\"Class constructor.\"\"\"\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, ins, batch_size):\n",
    "        \"\"\"Split inputs to heads\"\"\"\n",
    "        ins = tf.reshape(\n",
    "            ins, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(ins, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def __call__(self, ins):\n",
    "        \"\"\"Make it callable. Use scaled dot-product attention\n",
    "        and concatenate heads.\"\"\"\n",
    "        query, key, value, mask = ins['query'], ins['key'], ins[\n",
    "            'value'], ins['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = attention_weights(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        # final linear layer\n",
    "        outs = self.dense(concat_attention)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Add positional encoding to give more info about the relative position of the words in a sentence. The specific model by itself makes no assumptions about the spatial relationships across the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"Add positional encoding to give more info about the\n",
    "    relative position of the words in a sentence. The specific\n",
    "    model by itself makes no assumptions about the spatial\n",
    "    relationships across the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        \"\"\"Class constructor\"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        \"\"\"Calculate angle rads\"\"\"\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        \"\"\"Perform positional encoding\"\"\"\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # apply sin and cos to even and odd indexes in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Make it callable\"\"\"\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    \"\"\"The encoder layer consists of a multi head attention sublayer\n",
    "    and two dense layers followed by dropout.\n",
    "    \"\"\"\n",
    "    ins = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({'query': ins,\n",
    "                                               'key': ins,\n",
    "                                               'value': ins,\n",
    "                                               'mask': padding_mask\n",
    "                                               })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ins + attention)\n",
    "    outs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outs = tf.keras.layers.Dense(units=d_model)(outs)\n",
    "    outs = tf.keras.layers.Dropout(rate=dropout)(outs)\n",
    "    outs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outs)\n",
    "    return tf.keras.Model(inputs=[ins, padding_mask], outputs=outs, name=name)\n",
    "\n",
    "\n",
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    \"\"\"The encoder consists of the input embedding, the positional encoding\n",
    "    and the given number of layers.\n",
    "    \"\"\"\n",
    "    ins = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(ins)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    for i in range(num_layers):\n",
    "        outs = encoder_layer(units=units,\n",
    "                             d_model=d_model,\n",
    "                             num_heads=num_heads,\n",
    "                             dropout=dropout,\n",
    "                             name=\"encoder_layer_{}\".format(i),\n",
    "                             )([outs, padding_mask])\n",
    "    return tf.keras.Model(\n",
    "        inputs=[ins, padding_mask], outputs=outs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    \"\"\"The decoder layer consists of a masked multi head attention layer,\n",
    "    a multi head attention layer that receives the encoder output and the\n",
    "    output of the masked multi head attention sublayer, and two dense\n",
    "    layers followed by dropout.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")({'query': inputs,\n",
    "                                                 'key': inputs,\n",
    "                                                 'value': inputs,\n",
    "                                                 'mask': look_ahead_mask\n",
    "                                                 })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")({'query': attention1,\n",
    "                                                 'key': enc_outputs,\n",
    "                                                 'value': enc_outputs,\n",
    "                                                 'mask': padding_mask\n",
    "    })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='decoder'):\n",
    "    \"\"\"The decoder consists of output embedding, the positional encoding and the\n",
    "    given number of decoder layers.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer model\n",
    "\n",
    "The transformer consists of the encoder, decoder and a final linear layer. The output of the decoder constitutes the input to the final linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "    \"\"\"The transformer consists of the encoder, decoder and a final linear layer.\n",
    "    The output of the decoder constitutes the input to the final linear layer and\n",
    "    its output is returned.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
