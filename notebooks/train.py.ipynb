{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provides the functionality for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import tensorflow as tf\n",
    "from model import transformer\n",
    "from dataloader import load_data, build_tokenizer, tokenize, construct_input\n",
    "\n",
    "__author__ = \"ilias Zavitsanos\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"ilias Zavitsanos\"\n",
    "__email__ = \"izavits@gmail.com\"\n",
    "__status__ = \"Research Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "Use the Adam optimizer and the learning rate scheduler descibed in https://arxiv.org/abs/1706.03762\n",
    "The learning rate varies during training. Learning rate is increased linearly up to warm_up steps, and then slowly decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Set up the learning rate. Use the Adam optimizer and the learning\n",
    "    rate scheduler descibed in https://arxiv.org/abs/1706.03762\n",
    "    The learning rate varies during training.\n",
    "    Learning rate is increased linearly up to warm_up steps,\n",
    "    and then slowly decreased.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): number of features\n",
    "        warmup_steps (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        \"\"\"Class constructor\"\"\"\n",
    "        super(LearningRate, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"Make it callable\"\"\"\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the configuration from file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "seed = int(config['MODEL']['TfRandomSeed'])\n",
    "tf.random.set_seed(seed)\n",
    "datafile = '../' + config['DATA']['InputSet']\n",
    "\n",
    "# Get hyper-parameters\n",
    "NUM_LAYERS = int(config['MODEL']['NumLayers'])\n",
    "D_MODEL = int(config['MODEL']['Dmodel'])\n",
    "NUM_HEADS = int(config['MODEL']['NumHeads'])\n",
    "UNITS = int(config['MODEL']['Units'])\n",
    "DROPOUT = float(config['MODEL']['Dropout'])\n",
    "MAX_LENGTH = int(config['MODEL']['MaxLength'])\n",
    "EPOCHS = int(config['MODEL']['Epochs'])\n",
    "\n",
    "# Prepare data\n",
    "inputs, outputs = load_data(datafile)\n",
    "data_tokenizer, START_TOKEN, END_TOKEN, VOCAB_SIZE = build_tokenizer(inputs, outputs)\n",
    "questions, answers = tokenize(inputs, outputs)\n",
    "dataset = construct_input(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and define loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "    model = transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    \"\"\"Calculate the loss.\"\"\"\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set learning rate and optimizer and compile, fit and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up learning rate and optimizer before compiling and fitting the model\n",
    "learning_rate = LearningRate(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "# Save model config and weights\n",
    "model_name = config['DATA']['InputSet'].split('/')[-1].split('.')[0]\n",
    "# Save JSON config to disk\n",
    "json_config = model.to_json()\n",
    "with open('../models/'+model_name+'_config.json', 'w') as json_file:\n",
    "    json_file.write(json_config)\n",
    "# Save weights to disk\n",
    "model.save_weights('../models/'+model_name+'_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
